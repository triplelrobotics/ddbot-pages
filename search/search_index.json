{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Our robot project is named as \"ddbot\", standing for the ground mobile differntial-drive robot. We aim to make it AI-powered, affordable, along with extensive tutorials to aid the education in AI and robotics.</p> <p></p>"},{"location":"#blog","title":"Blog","text":"<p>Dev Update #5 (20240323)</p> <p>Dev Update #4 (20240224)</p> <p>Dev Update #3 (20240121)</p> <p>Dev Update #2 (20240113)</p> <p>Dev Update #1 (20240106)</p> <p>Introduce TripleL Robotics</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>We have released tutorials for ddbot. It's fun! Come and check out!</p> <p>Tutorials 01 Teleoperation</p> <p>Tutorials 02 Video Stream</p> <p>Tutorials 03 Collsion Avoidance</p> <p>Tutorials 04 Customized Algorithm</p>"},{"location":"blog/","title":"Posts","text":"<p>Our robot project is named as \"ddbot\", standing for the ground mobile differntial-drive robot. We aim to make it AI-powered, affordable, along with extensive tutorials to aid the education in AI and robotics.</p>"},{"location":"blog/2024/01/06/dev-update-1/","title":"Dev Update #1","text":"<p>This week we start the development of our first robot project, temporally named \"ddbot\", which is a ground robot with differential-drive kinematics. This robot is going to be our first product and also serves as a testbed for subsequent development.</p> <p>Progress: what we have done this week is to design and build up the first layer of chassis for the robot, as shown below.  </p> <p>We 3D print the base to support the battery, motors, and connectors. Right now the 1800mAh LIPO battery is used and we will see if it can last long enough for the applications. DC gearbox motors are used. Two main wheels and one universal caster wheel (in the front, below the base) are used for differential drive mode.</p> <p>Plan: for the next week, we try to set up the second layer of the chassis, adding the motor control board and an MCU for low-level control. Stay tuned...</p>"},{"location":"blog/2024/01/13/dev-update-2/","title":"Dev Update #2","text":"<p>This week we continue the development of our \"ddbot\" robot project.</p> <p>Progress: we have build up the second layer of chassis for the robot, as shown below.  </p> <p>We select a dual-channel DC motor drive module to drive two motors simultaneously and provide useful voltage outputs like 3.3V, 5V for other modules. We choose ESP32 as the microcontroller, which provides low-level interfaces such as I2C, SPI, PWM, etc. It can control devices with these interfaces, e.g. PWM for adjusting motor speed, I2C for reading IMU data. An 6-axis IMU sensor is included to measure car's attitude. For easy setup and flexibility, we use a breadboard for wire connections for now.</p> <p></p> <p>Plan: for the next week, we try to set up the third layer of the chassis, adding a camera module for vision-based tasks an a Pi (a.k.a. single board computer) for high-level central control. If time permits, we will add the fouth layer to include a 2D Lidar as well. Stay tuned...</p>"},{"location":"blog/2024/01/21/dev-update-3/","title":"Dev Update #3","text":"<p>This week we basically finish up building the hardware for the whole \"ddbot\" robot, including the third and fourth layer of its chassis.</p> <p>Progress: On the third layer, we add a OrangePi 3 LTS as the high-level central controller responsible for managing all peripherals, including sensors, and coordinating with other modules such as the ESP32 located on the second layer. It runs a modified verison of Linux Ubuntu operating system. Moreover, we attach a 8MP USB video camera (UVC) module on this layer at the very front of the robot body. This is required for all vision-based tasks. On the fourth layer, we add a Time-of-Flight (ToF)-based 2D LiDAR at the top for scanning the distance of nearby environment. This is useful for projects such as SLAM and collision avoidance. The robot is shown in the image below.  </p> <p>Plan: Over the following weeks, we shift the foucs to software development, from verifying all hardware modules to adding sevaral AI-based projects and their tutorials. In terms of the software proramming language, we will choose Python as it is widely used in AI &amp; deep learning community and is easy to learn. Specifically, we will use Python within the Linux OS on the OrangePi and use its micro-controller version, named CircuitPython, on the ESP32. Stay tuned ...</p>"},{"location":"blog/2024/02/24/dev-update-4/","title":"Dev Update #4","text":"<p>After a brief delay, we resume the development of ddbot, mainly focusing on software. To pointout where the software utilities are needed, we show a scatch of system structure here. The numbered arrows and blocks in red are where software should cover. These numbers help as reference to track the development status of each software component and are used below as [x] to match specific functionality.</p> <p>.</p> <p>Progress: We have developed and tested the software for several functionalities, including the wireless video streaming of UVC camera [1,5], the UART communication between Orangepi and ESP32 [6,7], a terminal UI-based control panel [4], and a simple motor control [8].</p> <p>Video streaming enables wireless, real-time visualization of the ddBot camera view on a webpage, facilitating remote FPV control. UART communication facilitates high- to low-level device information exchange, allowing the Orangepi to process compute-extensive AI tasks like image classification via deep learning, with its results sent to the ESP32 to control low-level hardware like motors. The terminal UI-based control panel offers users an easy way to control robot movement via terminal programs and keyboard inputs. Additionally, we do full voltage motor control using H-bridge mode. That works but leads to rapid movement and collision, causing the motor drive board burned up. We show the demos of these functions as follows.</p> <p>Wireless camera video streaming </p> <p>UART communication between Pi and ESP32: Pi send, ESP32 receive </p> <p>UART communication between Pi and ESP32: Pi receive, ESP32 send </p> <p>Terminal UI-based control panel: use local computer keyboard to send command to robot via ssh </p> <p>Simple but unfortunate motor control </p> <p>Plan: We need to reconfigure motor control software using PWM to limit robot speed and add safety measures. Also we will post the tutorials of current progress and integrate all software components together to have the first complete example.</p>"},{"location":"blog/2024/03/23/dev-update-5/","title":"Dev Update #5","text":"<p>In this update, we built a complete example where ddbot is controlled wirelessly to move around by a web-based joystick. </p> <p>Progress. Basically two main things have been done.</p> <ul> <li> <p>Reconfigured PWM-based motor control setup. We matched the rate voltage between motor (6V) and battery (12V) via PWM duty cycle adjustment to mitigate over-voltage risk, especially when motor reverses or brakes. Also introduced two modes for motor speed control: 'instant' (promptly update PWM signal) for responsive AI-driven applications and 'ramp' (gradually update PWM signal) for smoother human remote control scenarios.</p> </li> <li> <p>Enabled communication between robot and virtual joystick. We hosted a websocket server on ESP32 to allow fast data receiving from the webpage client, on where the joystick input is rendered and mapped to corresponded robot movement. This way we avoid compatibility with various gamepads and the joystick is equally accessible to any user with a mobile phone, tablet or PC device that has a web browser. </p> </li> </ul> <p>Look at the demos where ddbot moved around under manual control and saw the world with its onboard camera, plus another episode with third-person-view record!</p> <p>ddbot teleoperation via web-based joystick (First-Person-View) </p> <p>ddbot teleoperation via web-based joystick (Thrid-Person-View) </p> <p>Joystick control panel </p> <p>Plan. We will update the tutorials for the example above on ddbot documentation website. Also, we will start to build the first AI example on ddbot: using deep learning and computer vision for ddbot collision avoidance.</p>"},{"location":"blog/2024/01/05/introduce-triplel-robotics/","title":"Introduce TripleL Robotics","text":"<p>Welcome to TripleL Robotics, a robotic start-up working on building lower-cost robots with extensive AI capability for undergrad and grad robotic education and research. We will share our progress through weekly posts and any of your feedback is appreciated. :)</p> <p>Our mission is to enhance the robots with cutting-edge AI techniques such as deep learning and reinforcement learning to handle various tasks, including vision/lidar-based navigation, collision avoidance, hand gesture control, adaptive control in complex environments, etc. To achieve this, we will equip robots with powerful computing units as well as rich sensors such as IMUs, RGB-D cameras, Lidars, and ultrasonic modules. We will also open source easy-to-use software and comprehensive tutorials to reduce the learning curve. Our ultimate goal is to make these robots as central hardware for a series of AI robotic projects for educational purposes, at affordable prices.</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/dev-update/","title":"Dev Update","text":""},{"location":"blog/category/general/","title":"General","text":""}]}